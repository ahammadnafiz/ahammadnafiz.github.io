---
layout: post
title:  লজিস্টিক রিগ্রেশন - Under The Hood
date: 2024-05-19 9:30
categories: [Machine Learning Tutorial]
tags: [Machine Learning, tutorial]
---

এই আর্টিকেলে আমি লজিস্টিক রিগ্রেশন অ্যালগরিদমের কার্যপ্রণালী এবং এর অন্তর্নিহিত প্রক্রিয়াগুলি ব্যাখ্যা করার চেষ্টা করব।

![Logistic Regression](assets/Posts/Logistic Regression Article.png)

লজিস্টিক রিগ্রেশন একটি ক্লাসিফিকেশন অ্যালগরিদম। এটি আসলে একটি পদ্ধতি যা আমাদের জানায় কোনো একটি জিনিস ঘটবে কি ঘটবে না। যেমন, তোমার একটি খেলনা আছে যেটি দেখতে সুন্দর কি না, এই পদ্ধতি তা বলতে সাহায্য করতে পারে।

ধরো, আমরা জানতে চাই যে একটি ইমেল স্প্যাম (অপ্রয়োজনীয় বার্তা) কিনা। লজিস্টিক রিগ্রেশন আমাদের বলে দিতে পারে যে ইমেলটি স্প্যাম হওয়ার সম্ভাবনা কতটুকু। এটি 0 থেকে 1 এর মধ্যে একটি সংখ্যা দেয়, যেখানে 0 মানে কখনোই স্প্যাম নয় এবং 1 মানে নিশ্চিতভাবে স্প্যাম।

লজিস্টিক রিগ্রেশন ইনপুট ভেরিয়েবল এবং আউটপুটের মধ্যে সম্পর্ক তৈরি করতে একটি বিশেষ ফাংশন ব্যবহার করে, যাকে লজিস্টিক ফাংশন বা সিগময়েড ফাংশন বলা হয়। এটি এমনভাবে কাজ করে যে আউটপুট মান সবসময় 0 এবং 1 এর মধ্যে থাকে।

চলুন একটি উদাহরণ দেই:
ধরো তুমি জানতে চাও যে তোমার বন্ধু আজ খেলতে আসবে কিনা। যদি তোমার বন্ধুর মুড ভালো থাকে এবং সে মুক্ত থাকে, তবে তার আসার সম্ভাবনা বেশি। আমরা এই তথ্যগুলি (ইনপুট) নিয়ে লজিস্টিক রিগ্রেশন ব্যবহার করে বলতে পারি যে তার আসার সম্ভাবনা কতটুকু (আউটপুট)।

আচ্ছা, আরও একটি উদাহরণ দিয়ে এই বিষয়টি আরও সহজে বোঝার চেষ্টা করি।

ধরো, তুমি জানতে চাও তোমার বন্ধু পরীক্ষায় পাস করবে নাকি ফেল করবে। লজিস্টিক রিগ্রেশন এই বিষয়ে সাহায্য করতে পারে। এটি আমাদের জানায় যে তোমার বন্ধু পরীক্ষায় পাস করার সম্ভাবনা কতটুকু। এটি 0 থেকে 1 এর মধ্যে একটি সংখ্যা দেয়। এখানে 0 মানে সে পাস করবে না এবং 1 মানে সে পাস করবে।

ধরো, তুমি তোমার বন্ধু পরীক্ষায় জন্য ৩.২৫ ঘণ্টা পড়াশোনা করেছে।

![Example 1](assets/Posts/Logistic Regression Ex-1.png)

তাহলে,

No of Study Hours = 3.25

কিভাবে তুমি ফলাফল বের করবে? তুমি best fit line ব্যবহার করে এটি বের করতে পারো।
- প্রথমে পড়াশোনার সময়ের অক্ষ (3.25 বিন্দু) থেকে সেরা ফিট লাইনে একটি লাইন টেনে আনো।
- তারপর সেরা ফিট লাইন থেকে আউটকামের অক্ষে আরেকটি লাইন টেনে আনো।

তাহলে তুমি একটি মান পাবে, ধরো এটি 0.7 হয়।

তাহলে,


```
    hθ(x) < 0.5 হলে ---> আমার আউটপুট হবে 0 (ফেল)

    hθ(x) ≥ 0.5 হলে ---> আমার আউটপুট হবে 1 (পাস)
```


এখন, যদি তোমার বন্ধুর ৩.২৫ ঘণ্টা পড়াশোনার পর  h*θ(x) = 0.7*  হয়, তাহলে আউটপুট হবে 1 (পাস)। অর্থাৎ, তোমার বন্ধু পরীক্ষায় পাস করবে।

- তুমি জানো যে 0.5 বা তার বেশি মানে পাস →  ফলাফল 1।

**তাহলে লিনিয়ার রিগ্রেশনের সাথে বাইনারি ক্লাসিফিকেশনের সমস্যা কোথায়।**

ধরো,  তোমার আরেক বন্ধু পরীক্ষায় পাস করবে নাকি ফেল করবে তা জানতে চাও এবং সে ৭ ঘণ্টা পড়াশোনা করেছে। ধরে নিই যে ৭ ঘণ্টা পড়াশোনা করে সে পাস করেছে, অর্থাৎ আউটকাম = 1।

এখন, যদি আমরা সেরা ফিট লাইন আঁকি, তাহলে আমরা একটি নতুন সেরা ফিট লাইন পাবো (লাল লাইন)।

এখন, যদি তুমি আগের বন্ধুর পাস বা ফেল হওয়ার অবস্থা চেক করতে চাও যখন সে ৩.২৫ ঘণ্টা পড়াশোনা করেছে:

![Example 2](assets/Posts/Logistic Regression Ex-2.png)

No of Study Hours = 3.25

তুমি কীভাবে আউটকাম পাবে? তুমি খুব সহজেই তোমার সেরা ফিট লাইন ব্যবহার করে পেতে পারো।

- প্রথমে পড়াশোনার সময়ের অক্ষ (3.25 বিন্দু) থেকে সেরা ফিট লাইনে একটি লাইন টেনে আনো।
- তারপর সেরা ফিট লাইন থেকে আউটকামের অক্ষে আরেকটি লাইন টেনে আনো।

এভাবে, তুমি একটি মান পাবে, যা 0.464 হতে পারে।
- তুমি জানো যে 0.5 এর নিচে হলে ফেল হিসাবে বিবেচনা করা হয় → ফলাফল 2।

এখন, ফলাফল 1 এবং ফলাফল 2 তুলনা করে দেখলে, তুমি দেখতে পাবে যে ৩.২৫ ঘণ্টা পড়াশোনার জন্য তুমি দুটি ভিন্ন উত্তর পেয়েছো। এছাড়াও, কখনও কখনও তুমি আউটকাম <0 এবং >1 মানও পেতে পারো। তাই লিনিয়ার রিগ্রেশন বাইনারি ক্লাসিফিকেশন সমস্যার ক্ষেত্রে সঠিক উত্তর দেয় না।

তাহলে, আমরা কীভাবে ক্লাসিফিকেশন সমস্যার সমাধান করতে পারি? আমরা এই সমস্যার জন্য সিগময়েড ফাংশন ব্যবহার করতে পারি।

### লজিস্টিক রিগ্রেশনের ডিসিশন বাউন্ডারি


```
hθ(x) = θ0 + θ1x1 + θ2x2 + .. + θnxn

hθ(x) = θTx
```


লজিস্টিক রিগ্রেশনের ডিসিশন বাউন্ডারি বুঝতে গেলে আমাদের সেরা ফিট লাইনকে একটু পরিবর্তন করতে হবে। এজন্য আমরা সিগময়েড ফাংশন ব্যবহার করতে পারি।

Straight line equation : __hθ(x) = θ0 + θ1x1__

আমরা এই লিনিয়ার রিগ্রেশন সমীকরনের উপর একটি গাণিতিক ফর্মুলা (g) ব্যবহার করে এই রেখাকে পরিবর্তন করতে পারি।


```
hθ(x) = g (θ0 + θ1x)

Let z = θ0 + θ1x

hθ(x) = g (z) এটাই আমরা ব্যবহার করি, সিগময়েড ফাংশন (লজিস্টিক ফাংশন)।

hθ(x) = 1 / 1+e^(-z)

hθ(x) = 1 / 1+e^(-(θ0 + θ1x)) : এটাই হলো আমাদের হাইপোথিসিস।

সিগময়েড ফাংশন (লজিস্টিক ফাংশন)
----------------------------
g (z) = 1 / 1+e^(-z)

```

এখন চলো উদাহরণ সহ দেখি। ধরো, আমরা আগের মতই পড়ার  সময় নিয়ে কাজ করছি। যদি আমরা ৩.২৫ ঘণ্টা পড়াশোনা করেছি, তাহলে 

__z = θ0 + θ1*3.25__ 

হবে। তারপর hθ(x) বা আউটকাম পেতে, আমরা এই মানটি সিগময়েড ফাংশনে বসাবো।

সিগময়েড ফাংশন আউটকামকে 0 থেকে 1 এর মধ্যে স্কোয়াশ করবে, তাই আউটকাম সবসময় 0 এবং 1 এর মধ্যে থাকবে। এর ফলে, আমরা সহজেই বুঝতে পারবো যে ছাত্রটি পাস করবে নাকি ফেল করবে। 0.5 বা তার বেশি মানে পাস এবং কম হলে ফেল।

সিগময়েড ফাংশনের প্লট দেখতে এরকম হয়:

__g (z) = 1 / 1+e^(-z)__

এখন সিগময়েড ফাংশনের প্লটটি বর্ণনা করি। এটি একটি __'S'__ আকৃতির বক্ররেখা যা __z__ এর মানের ওপর নির্ভর করে। এই বক্ররেখাটি সবসময় 0 থেকে 1 এর মধ্যে থাকে।

**সিগময়েড ফাংশনের প্লট:**

![Example 3](assets/Posts/Logistic Regression Ex-3.png)

**বৈশিষ্ট্য:**

1. যখন z এর মান খুব ছোট (বেশি নেতিবাচক) হয়, g(z) প্রায় 0 এর সমান।
2. যখন z এর মান খুব বড় (বেশি ধনাত্মক) হয়, g(z) প্রায় 1 এর সমান।
3. যখন z = 0, তখন __g(z)__ = 0.5।

এখন, সিগময়েড ফাংশন আমাদের লজিস্টিক রিগ্রেশন মডেলকে একটি নির্দিষ্ট পরিসরে রাখে, যাতে আউটকাম সবসময় 0 এবং 1 এর মধ্যে থাকে। এর ফলে আমরা সহজেই বুঝতে পারি যে কোন আউটকাম পাস এবং কোনটি ফেল।

এভাবে, লজিস্টিক রিগ্রেশন এবং সিগময়েড ফাংশন ব্যবহার করে আমরা সহজেই বাইনারি ক্লাসিফিকেশন সমস্যার সমাধান করতে পারি।

লজিস্টিক রিগ্রেশন প্রয়োগের জন্য একটি ভিন্ন কস্ট ফাংশন প্রয়োজন, কারণ লজিস্টিক রিগ্রেশনের জন্য সঠিক কস্ট ফাংশনটি লিনিয়ার রিগ্রেশনের কস্ট ফাংশনের মতো নয়। চলো সহজভাবে দেখে নিই কিভাবে এটি কাজ করে।

### প্রাথমিক ধারণা

গুরুত্বপূর্ণ অনুমান:

```
g (z) ≥ 0.5 যখন z ≥0
```

ধরো, আমাদের কিছু ট্রেনিং সেট আছে:

```

{(x1,y1), (x2,y2), (x3,y3), (x4,y5). . . . . . (xn, yn)}

y ∈ {0, 1} ; 2 Outputs
```

কারণ আমরা একটি বাইনারি ক্লাসিফিকেশন সমস্যার সমাধান করছি।

### লজিস্টিক রিগ্রেশন মডেল

__g (z) = 1 / 1+e^(-z)__

আমরা এটি পরিবর্তন করে লিখতে পারি:

```
hθ(x) = 1 / 1+e^(-z) 
যেখানে, z = θ0 + θ1x
```

ধরুন এই গ্রাফটি অরিজিনের মধ্য দিয়ে যায়, তাহলে আমরা ধরে নিতে পারি __θ0 = 0__

```
hθ(x) = 1 / 1+e^(-z)
যেখানে , z = θ1x
```

প্যারামিটার θ1 পরিবর্তন করি এবং সিগময়েড অ্যাক্টিভেশন ফাংশনটি সেরা ফিট লাইনে প্রয়োগ করি।

### কস্ট ফাংশন (Cost Function)

লিনিয়ার রিগ্রেশনের জন্য কস্ট ফাংশন:

```
J(θ0,θ1) =  1/2m [i =1 -> m Σ (hθ(x)^i - y^i)^2] 
```

### লজিস্টিক রিগ্রেশন

লজিস্টিক রিগ্রেশনের হাইপোথিসিস:

```
hθ(x) = 1 / 1+e^(-θ1x) 
```

যেহেতু শুধুমাত্র hθ পরিবর্তিত হচ্ছে, তাই আমরা কস্ট ফাংশন পুনরায় তৈরি করতে পারি:

```
J(θ0,θ1) =  1/2m (hθ(x)^i - y^i)^2
```

কিন্তু, আমরা লজিস্টিক রিগ্রেশনের জন্য এই ধরনের কস্ট ফাংশন প্রয়োগ করতে পারি না কারণ লজিস্টিক রিগ্রেশনের জন্য সঠিক কস্ট ফাংশনটি ভিন্ন।

### লজিস্টিক রিগ্রেশনের সঠিক কস্ট ফাংশন

লজিস্টিক রিগ্রেশনের জন্য, আমরা যে কস্ট ফাংশন ব্যবহার করি তা হলো:

```
J(θ0,θ1) =  1/2m (hθ(x)^i - y^i)^2

        =  1/2m (1 / (1+e^(-θ1x))^i - y^i)^2
```

এই কস্ট ফাংশনটি মডেলটিকে আরও ভালভাবে প্রশিক্ষণ দেয় এবং লজিস্টিক রিগ্রেশনের জন্য সবচেয়ে উপযুক্ত। এটি নিশ্চিত করে যে আমাদের মডেলটি সঠিকভাবে কাজ করছে এবং যথাযথভাবে ক্লাসিফিকেশন করছে।

সুতরাং, লজিস্টিক রিগ্রেশনের জন্য আমাদের এই বিশেষ কস্ট ফাংশন প্রয়োজন, যা লিনিয়ার রিগ্রেশনের কস্ট ফাংশনের মতো নয়।

## কনভেক্স এবং নন-কনভেক্স ফাংশনের মধ্যে পার্থক্য

কনভেক্স এবং নন-কনভেক্স ফাংশন মেশিন লার্নিংয়ে বিশেষত অপ্টিমাইজেশন সমস্যায় গুরুত্বপূর্ণ ভূমিকা পালন করে।

### কনভেক্স ফাংশন

- **ইউনিক গ্লোবাল মিনিমাম**: কনভেক্স ফাংশনের একটি মাত্র গ্লোবাল মিনিমাম থাকে। এর ফলে অপ্টিমাইজেশন প্রক্রিয়া সহজ এবং নির্ভরযোগ্য হয়।
- **পরিবর্তনের নিয়ম**: যদি তুমি দুটি পয়েন্ট __x_1__ এবং __x_2__ নাও এবং একটি লাইন ড্র করো __f(x_1)__ এবং __f(x_2)__ এর মধ্যে, তবে ফাংশনের সকল মান সেই লাইনের নিচে থাকবে।
- **অপ্টিমাইজেশন সহজ**: যেকোনো লোকাল মিনিমাম গ্লোবাল মিনিমামই হবে। তাই আমরা সহজেই সঠিক সমাধান পেতে পারি।

### নন-কনভেক্স ফাংশন

- **মাল্টিপল লোকাল মিনিমা**: নন-কনভেক্স ফাংশনে একাধিক লোকাল মিনিমা থাকতে পারে। এর ফলে অপ্টিমাইজেশন প্রক্রিয়া কঠিন হয়ে যায়।
- **গ্লোবাল মিনিমা খুঁজে পাওয়া কঠিন**: নন-কনভেক্স ফাংশনে গ্লোবাল মিনিমা খুঁজে পাওয়া কঠিন, কারণ তুমি লোকাল মিনিমাতে আটকে যেতে পারো।
- **বিভিন্ন মান**: যদি তুমি দুটি পয়েন্ট __x_1__ এবং __x_2__ নাও এবং একটি লাইন ড্র কর __f(x_1)__ এবং __f(x_2)__ এর মধ্যে, তবে ফাংশনের কিছু মান সেই লাইনের উপরে থাকতে পারে।

### উদাহরণ

**কনভেক্স ফাংশন:**
__f(x) = x^2__

এই ফাংশনের প্লট একটি 'U' আকৃতি, যা কনভেক্স ফাংশনের বৈশিষ্ট্য।

**নন-কনভেক্স ফাংশন:**
__f(x) = x^4 - 4x^2__

এই ফাংশনের প্লট অনেকগুলো লোকাল মিনিমা এবং একটি গ্লোবাল মিনিমা থাকতে পারে, যা নন-কনভেক্স ফাংশনের বৈশিষ্ট্য।

### চিত্র

![Example 4](assets/Posts/Logistic Regression Ex-4.png)

কনভেক্স ফাংশন অপ্টিমাইজ করা সহজ এবং নির্ভরযোগ্য, কারণ এতে একটি মাত্র গ্লোবাল মিনিমাম থাকে। অন্যদিকে, নন-কনভেক্স ফাংশনে একাধিক লোকাল মিনিমা থাকতে পারে, যা অপ্টিমাইজেশন প্রক্রিয়াকে কঠিন করে তোলে। তাই মেশিন লার্নিংয়ে কনভেক্স ফাংশন বেশি সুবিধাজনক এবং ব্যবহার করা হয়।

নন-কনভেক্স ফাংশন সমস্যার সমাধান করতে আমরা লজিস্টিক রিগ্রেশনের কস্ট ফাংশন ব্যবহার করতে পারি। লজিস্টিক রিগ্রেশনের জন্য সঠিক কস্ট ফাংশনটি এমনভাবে ডিজাইন করা হয়েছে যাতে এটি কনভেক্স হয়। এর ফলে অপ্টিমাইজেশন প্রক্রিয়া সহজ এবং নির্ভরযোগ্য হয়।

### লজিস্টিক রিগ্রেশনের কস্ট ফাংশন

লজিস্টিক রিগ্রেশনের জন্য কস্ট ফাংশনটি নিচের মতো:

```
J(θ1) =  [ -log(hθ(x)^i)       ; যদি y = -1  তাহলে এই সমীকরনটি ব্যাবহার করব

          -log(1 - hθ(x)^i)    ; যদি y = ০  তাহলে এই সমীকরনটি ব্যাবহার করব ]

```

এই ফাংশনটি আমরা ব্যবহার করি, যেখানে hθ(x) হল আমাদের হাইপোথিসিস ফাংশন যা সিগময়েড ফাংশনের মাধ্যমে নির্ধারিত হয়:

__hθ(x) = 1 / 1+e^(-z)__

### উদাহরণ সহ ব্যাখ্যা

ধরা যাক আমাদের ডেটা সেটের একটি ডেটা পয়েন্ট (x, y), যেখানে Y হয় 0 অথবা 1।

- যদি Y = -1, তাহলে কস্ট ফাংশনটি হবে:

```
    -log(hθ(x)^i)
```

![Example 5](assets/Posts/Logistic Regression Ex-5.png)


- যদি Y = 0, তাহলে কস্ট ফাংশনটি হবে:

```
    -log(1 — hθ(x)^i)
```

![Example 6](assets/Posts/Logistic Regression Ex-6.png)


### কস্ট ফাংশন

সম্পূর্ণ ডেটা সেটের উপর ভিত্তি করে কস্ট ফাংশনটি নির্ধারণ করা হয়:

```
Cost (hθ(x)^i, y) =  [ -log(hθ(x)^i)       ; যদি y = -1  তাহলে এই সমীকরনটি ব্যাবহার করব

                        -log(1 - hθ(x)^i)    ; যদি y = ০  তাহলে এই সমীকরনটি ব্যাবহার করব ]

Cost (hθ(x)^i, y) =  -y log(hθ(x)^i) - (1-y) log(1 - hθ(x)^i)
```

### সুবিধা

- **কনভেক্স ফাংশন**: এই কস্ট ফাংশনটি কনভেক্স, যার ফলে এটি সহজেই অপ্টিমাইজ করা যায় এবং গ্লোবাল মিনিমা খুঁজে পাওয়া যায়।
- **স্ট্যাবিলিটি**: এই ফাংশনটি স্ট্যাবিলিটি প্রদান করে, যার

### লজিস্টিক রিগ্রেশনের কনভার্জেন্স অ্যালগরিদম প্রয়োগ করা

লজিস্টিক রিগ্রেশনের কস্ট ফাংশনকে কনভার্জেন্স অ্যালগরিদমে প্রয়োগ করতে পারি। চলুন ধাপে ধাপে বুঝে নেই।

### কস্ট ফাংশন

```
লজিস্টিক রিগ্রেশনের কস্ট ফাংশনটি হলো:

Cost (hθ(x)^i, y) =  -y log(hθ(x)^i) - (1-y) log(1 - hθ(x)^i)

যদি y = 1, তাহলে কস্ট ফাংশনটি হবে:

Cost (hθ(x)^i, y) =  -1 log(hθ(x)^i) - (0) log(1 - hθ(x)^i)

=  - log(hθ(x)^i)

যদি y = 0, তাহলে কস্ট ফাংশনটি হবে:

Cost (hθ(x)^i, y) =  -0 log(hθ(x)^i) - (1-0) log(1 - hθ(x)^i)

=  - log(1 - hθ(x)^i)
```

### সম্পূর্ণ কস্ট ফাংশন

```
J(θ1) =  1/2m [i =1 -> m Σ -y log(hθ(x)^i) - (1-y) log(1 - hθ(x)^i)]

= - 1/2m [i =1 -> m Σ y^i log(hθ(x)^i) + (1-y^i) log(1 - hθ(x)^i)]
```

### লজিস্টিক রিগ্রেশনের হাইপোথিসিস

__hθ(x) = 1 / 1+e^(-θ1x)__

### কনভার্জেন্স অ্যালগরিদম

কনভার্জেন্স অ্যালগরিদম ব্যবহার করে আমরা অপ্টিমাইজেশন করতে পারি। এটি একটি গ্র্যাডিয়েন্ট ডিজেন্ট অ্যালগরিদম যা নীচের মত কাজ করে:

```
θ0 = θ0 - α (d(J(θ1))/ dθj)
```

যেখানে alpha হলো লার্নিং রেট।

### পুনরাবৃত্তি কনভার্জেন্স পর্যন্ত

এখন আমরা পুরো প্রক্রিয়াটি পুনরাবৃত্তি করবো যতক্ষণ না কনভার্জেন্স ঘটে:

```
Repeat until Convergence

{

θ0 := θ0 - α (d(J(θ1))/ dθj)

}
```

### উপসংহার

এটি ছিল লজিস্টিক রিগ্রেশন সম্পর্কে একটি মৌলিক আলোচনা। এই আলোচনার মাধ্যমে আমরা লজিস্টিক রিগ্রেশনের কস্ট ফাংশন এবং কনভার্জেন্স অ্যালগরিদম সম্পর্কে জানতে পারলাম। আশা করি তুমি এটি বুঝতে পেরেছো। পরবর্তী আলোচনায় আমরা  কিভাবে এটিকে কোড করতে হয় তা নিয়ে আলোচনা করবো। আবার দেখা হবে আরেকটি টিউটোরিয়ালে।